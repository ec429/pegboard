init.s gets built to address 0.  All CPUs will enter it as soon as they come online; and they will all have the same SP, so init.s can't touch the stack.  (Until the SP fixup, we never CALL or PUSH.)  When init is done (and calls main), each CPU has its own 32-byte stack, at the top of which is a struct percpu_struct (a pointer to this is normally kept in IY).  This stack will be used when not in any process (i.e. during the scheduler, or when an interrupt is taken while the CPU has no process runnable).

spinlock.s provides a simple spinlock based around the instruction 'lock sra (IX+0)'.  It was inspired by http://u6c87.blogspot.co.uk/2011/10/atomic-instructions-on-z80-for-locking.html but that article only considers UP; for SMP the lock prefix had to be added to ensure the MMU doesn't let someone else in during our read-modify-write.  Spinlocks tend to perform poorly under contention, and this is certainly the case here; since a locked op blocks _all_ other memory traffic, a herd of CPUs trying to acquire the lock can significantly slow down the release path, leading to O(nÂ²) thrashing.  Fortunately, it should be possible to loosen the locked-op rules while maintaining correctness, at the cost of increased MMU complexity; this could reduce the performance penalty significantly.

Allocations are stored in a bitmap char mem_map[255], where mem_map[i] contains the PID of the owner of page (i+1) (or 0 for free pages).  Page 0 is always owned by the kernel (and incidentally, contains the mem_map bitmap).  We also have the lock (spinlock_t mem_lock), and the number of free pages (char mem_free).  If there are less than 256 pages of memory attached, let's say there are 64, then only the first 63 bytes of mem_map are meaningful, and mem_free only counts 0s in those 63 bytes.  This means that, by (a) only allocating a page if mem_free > 0 and (b) always allocating the lowest-numbered available page, only pages that are attached will ever be allocated.
Note that NULL points to the start of init, and reads/writes through it will not fault or trap.  (This may change in the future if page 0 contains only .text and is thus set up to be read-only.)

For now, scheduling is based on a simple run-queue; any process which expires its time-slice gets added to the tail; and the next process to run is popped from the head.  The queue is a fixed 8-slot allocation (so this will be our max. number of processes.  It can be changed by altering PROC_SLOTS in sched.s).
enum status_t {
	TASK_RUNNING=0, /* currently running on a CPU */
	TASK_RUNNABLE=1, /* waiting for a timeslot */
	TASK_INTERRUPTIBLE=2, /* sleeping, but can be killed */
	TASK_UNINTERRUPTIBLE=3, /* sleeping, cannot be killed */
};
Note a special case: when a process fork()s, the child may be TASK_UNINTERRUPTIBLE while its state is copied from the parent.  If the parent happens to expire its time-slice while doing this, the child will remain in TASK_UNINTERRUPTIBLE until the parent can finish.  What's really happening here is that the child doesn't yet have an execution context, so its status is tied to that of the parent.
struct list_head runq; /* processes in TASK_RUNNABLE */
struct process { struct list_head runq; byte pid; enum status_t status; byte basepage; byte ppid; } procs[8];
I did wonder if we needed to save the registers in struct process as well, but in fact we can save them on the kernel stack, which will be on the basepage, along with other per-process data such as page allocations.
pid 0 represents "no process".
Any process in TASK_[UN]INTERRUPTIBLE should be tied to some event that it's waiting for.  In some cases this will be a struct list_head waitq associated with whatever can trigger that event.  (However, we don't yet have anything sleepable-on except for the fork() case above, which only one process - the new child - can be waiting for.)

***PROBLEM: IM 2 is nonviable because user code could change the I register and redirect interrupts.  POSSIBLE SOLUTION: the MMU could recognise the LD I,A opcode (I believe it's the only way to set I) and block it (turn it into a NOP or TRAP) if the PC isn't in a ppage with PROT_KERNEL***

Process stack page layout:
	0000: Saved SP (word).  While process is running, instead contains the saved per-cpu SP (which is in the per-cpu data area).
	0002: VM maps (byte[16])
	0012:
	Stack starts at top of page (and grows down).  On yield/swap, push AF BC DE HL IX and save SP (we do not save shadow registers, nor IY which should always point to the percpu_struct).

Next steps:
	Implement sleeping locks (semaphores).
	Define a simple virtual disk device, and write a driver for it.
	Implement a kmalloc/kfree.
	Write useful print routines (a simple printk?)
	Devise a scheme for IPIs.  (Probably just an OUT whose high address byte is CPUid, and the data byte could be a mailbox number of some sort.)
	Write a panic() function, for when things go wrong.
	Design a system call entry mechanism that allows for protection (eg. prevent mapping in kernel pages except by sysenter.  Maybe use RST 38 as SYSENTER.  This, and interrupts, need to automatically page in 0 at 0).
	Pick an executable format and write a program loader (perhaps also a dynamic linker?)
